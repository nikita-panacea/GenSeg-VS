{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa22064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from options.train_options import TrainOptions\n",
    "from data import create_dataset\n",
    "from data.vs_dataset import VSDataset\n",
    "from models import create_model\n",
    "from util.util import print_timestamped\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c92f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_entries(csv_path, image_col='image_nifti', mask_col='mask_nifti', id_col='patient_id'):\n",
    "    entries = []\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            img = row.get(image_col)\n",
    "            msk = row.get(mask_col)\n",
    "            pid = row.get(id_col, '')\n",
    "            if img and msk:\n",
    "                entries.append({'image': img, 'mask': msk, 'id': pid})\n",
    "    return entries\n",
    "\n",
    "def split_entries(entries, train_frac=0.8, seed=42):\n",
    "    random.Random(seed).shuffle(entries)\n",
    "    n = int(len(entries) * train_frac)\n",
    "    return entries[:n], entries[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f7c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--csv', default='D:/GenSeg-VS/GenSeg_VS.csv', help='CSV with image and mask columns')\n",
    "parser.add_argument('--checkpoints_dir', default='./checkpoints', help='where to save models')\n",
    "parser.add_argument('--name', default='vs_experiment', help='experiment name')\n",
    "parser.add_argument('--gpu_ids', default='0', help='gpus')\n",
    "parser.add_argument('--train_frac', type=float, default=0.8)\n",
    "parser.add_argument('--selected_radiomics', type=str, default='', help='comma-separated names to use (empty = all)')\n",
    "parser.add_argument('--mask_aug', type=str, default='none', choices=['none', 'dilate', 'erode'])\n",
    "parser.add_argument('--use_pyrad', action='store_true')\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bdc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "cleaned_argv = [sys.argv[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec796fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while i < len(sys.argv):\n",
    "    a = sys.argv[i]\n",
    "    # skip '--csv value' or '--csv=value'\n",
    "    if a == '--csv' or a.startswith('--csv='):\n",
    "        if a == '--csv':\n",
    "            i += 2\n",
    "            continue\n",
    "        else:\n",
    "            i += 1\n",
    "            continue\n",
    "    cleaned_argv.append(a)\n",
    "    i += 1\n",
    "sys.argv = cleaned_argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = TrainOptions().initialize(argparse.ArgumentParser())\n",
    "opt = parser.parse_args([\n",
    "    '--dataroot', 'D:/GenSeg-VS/Data',\n",
    "    '--dataset_mode', 'nifti',\n",
    "    '--model', 'pix2pix3d_radiomics',\n",
    "    '--name', 'liver-98',\n",
    "    '--gpu_ids', '0'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f493fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = TrainOptions().parse()\n",
    "# # opt, _ = parser.parse_known_args('--dataroot', 'path/to/data',\n",
    "# #                             '--model', 'pix2pix3d')\n",
    "# opt = TrainOptions().parse(['--dataroot', 'path/to/data',\n",
    "#                             '--model', 'pix2pix3d'])\n",
    "\n",
    "\n",
    "\n",
    "opt.checkpoints_dir = args.checkpoints_dir\n",
    "opt.name = args.name\n",
    "# parse gpu ids string into list of ints (same logic as BaseOptions.parse)\n",
    "# str_ids = str(args.gpu_ids).split(',') if args.gpu_ids is not None else []\n",
    "# opt.gpu_ids = []\n",
    "# for str_id in str_ids:\n",
    "#     try:\n",
    "#         _id = int(str_id)\n",
    "#     except Exception:\n",
    "#         continue\n",
    "#     if _id >= 0:\n",
    "#         opt.gpu_ids.append(_id)\n",
    "# print(opt.gpu_ids)\n",
    "# if len(opt.gpu_ids) > 0:\n",
    "#     torch.cuda.set_device(opt.gpu_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c47b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.vs_entries = None\n",
    "opt.use_pyradiomics = args.use_pyrad\n",
    "opt.selected_radiomics = args.selected_radiomics\n",
    "opt.mask_aug = args.mask_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10db5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = load_csv_entries(args.csv)\n",
    "train_e, val_e = split_entries(entries, train_frac=args.train_frac, seed=args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0d07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances manually and wrap them into the data loader interface\n",
    "# We will create two datasets and two dataloaders and perform a simplified training loop\n",
    "opt.phase = 'train'\n",
    "opt.vs_entries = train_e\n",
    "train_dataset = VSDataset(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be1cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.phase = 'val'\n",
    "opt.vs_entries = val_e\n",
    "val_dataset = VSDataset(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9357ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=int(opt.num_threads))\n",
    "val_loader = DataLoader(val_dataset, batch_size=opt.batch_size, shuffle=False, num_workers=int(opt.num_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "383fd0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 81, Val size: 21\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4541e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.model= 'pix2pix3d_radiomics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35e5d1f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'radiomics.features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m opt\u001b[38;5;241m.\u001b[39misTrain\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      2\u001b[0m opt\u001b[38;5;241m.\u001b[39mgpu_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msetup(opt)\n",
      "File \u001b[1;32md:\\GenSeg-VS\\GenSeg-3D\\models\\__init__.py:64\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_model\u001b[39m(opt):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a model given the option.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    This function warps the class CustomDatasetDataLoader.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        >>> model = create_model(opt)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mfind_model_using_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     instance \u001b[38;5;241m=\u001b[39m model(opt)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] was created\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(instance)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32md:\\GenSeg-VS\\GenSeg-3D\\models\\__init__.py:33\u001b[0m, in \u001b[0;36mfind_model_using_name\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Import the module \"models/[model_name]_model.py\".\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03mIn the file, the class called DatasetNameModel() will\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03mbe instantiated. It has to be a subclass of BaseModel,\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03mand it is case-insensitive.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 33\u001b[0m modellib \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     35\u001b[0m target_model_name \u001b[38;5;241m=\u001b[39m model_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32md:\\GenSeg-VS\\GenSeg-3D\\models\\pix2pix3d_radiomics_model.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpix2pix3d_radiomics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pix2Pix3DRadiomicsModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This file exists to satisfy the model discovery convention:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#   --model pix2pix3d_radiomics  -> models.pix2pix3d_radiomics_model  -> Pix2Pix3DRadiomicsModel\u001b[39;00m\n",
      "File \u001b[1;32md:\\GenSeg-VS\\GenSeg-3D\\models\\pix2pix3d_radiomics.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpix2pix3d_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pix2Pix3DModel\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m networks\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mradiomics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m masked_tensor_stats, features_to_vector, normalize_feature_vector, augment_mask_binary\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPix2Pix3DRadiomicsModel\u001b[39;00m(Pix2Pix3DModel):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    Pix2Pix3D augmented with a radiomics feature matching loss between\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    (real_B, augmented_mask) and (fake_B, augmented_mask).\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    Loss: L_total = L_GAN + lambda_L1 * L1(masked) + gamma_TMSE * L2_T + lambda_rad * MSE(f_real, f_fake)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'radiomics.features'"
     ]
    }
   ],
   "source": [
    "opt.isTrain= True\n",
    "opt.gpu_ids = [0]\n",
    "model = create_model(opt)\n",
    "model.setup(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e17825a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iters = 0\n",
    "import collections\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1155935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pix2pix] real_A: (1, 1, 64, 64, 64), real_B: (1, 1, 64, 64, 64), mask: (1, 1, 64, 64, 64), truth: (1, 1, 64, 64, 64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7, 7], expected input[1, 1, 70, 70, 70] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m total_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mset_input(data)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss_info \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_losses_no_backward()\n",
      "File \u001b[1;32md:\\GenSeg-VS\\GenSeg-3D\\models\\pix2pix_model.py:150\u001b[0m, in \u001b[0;36mPix2PixModel.forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfake_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetG\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal_A\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# G(A) A: [1, 1, 64, 64, 64] G(A): [1, 1, 64, 64, 64]\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# One-time debug print of generated mini-batch shape\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape_debug_printed:\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:166\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\GenSeg-VS\\GenSeg-3D\\models\\networks.py:495\u001b[0m, in \u001b[0;36mResnetGenerator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Standard forward\"\"\"\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\modules\\conv.py:607\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\anaconda3\\envs\\GenSeg\\lib\\site-packages\\torch\\nn\\modules\\conv.py:602\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    592\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    593\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7, 7], expected input[1, 1, 70, 70, 70] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.train()\n",
    "train_sums = collections.defaultdict(float)\n",
    "train_batches = 0\n",
    "for i, data in enumerate(train_loader):\n",
    "    total_iters += opt.batch_size\n",
    "    model.set_input(data)\n",
    "    model.forward()\n",
    "    loss_info = model.compute_losses_no_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f788512",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(opt\u001b[38;5;241m.\u001b[39mepoch_count, opt\u001b[38;5;241m.\u001b[39mn_epochs \u001b[38;5;241m+\u001b[39m opt\u001b[38;5;241m.\u001b[39mn_epochs_decay \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Training epoch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m     train_sums \u001b[38;5;241m=\u001b[39m \u001b[43mcollections\u001b[49m\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      5\u001b[0m     train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):\n",
    "    # Training epoch\n",
    "    model.train()\n",
    "    train_sums = collections.defaultdict(float)\n",
    "    train_batches = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        total_iters += opt.batch_size\n",
    "        model.set_input(data)\n",
    "\n",
    "#         # Forward and compute losses without performing backward to detect instability\n",
    "#         try:\n",
    "#             model.forward()\n",
    "#             loss_info = model.compute_losses_no_backward()\n",
    "#         except Exception as e:\n",
    "#             print_timestamped(f\"Error computing losses on batch {i}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # check for NaN/Inf in computed losses\n",
    "#         bad = False\n",
    "#         for v in loss_info.values():\n",
    "#             try:\n",
    "#                 if np.isnan(v) or np.isinf(v):\n",
    "#                     bad = True\n",
    "#                     break\n",
    "#             except Exception:\n",
    "#                 # non-scalar entries\n",
    "#                 pass\n",
    "#         if bad:\n",
    "#             print_timestamped(f\"NaN/Inf detected in losses on batch {i}, skipping optimizer step\")\n",
    "#             continue\n",
    "\n",
    "#         # Safe to apply optimizer steps (mirror optimize_parameters)\n",
    "#         try:\n",
    "#             # update D\n",
    "#             model.set_requires_grad(model.netD, True)\n",
    "#             model.optimizer_D.zero_grad()\n",
    "#             model.backward_D()\n",
    "#             model.optimizer_D.step()\n",
    "\n",
    "#             # update G\n",
    "#             model.set_requires_grad(model.netD, False)\n",
    "#             model.optimizer_G.zero_grad()\n",
    "#             model.optimizer_arch.zero_grad()\n",
    "#             model.backward_G()\n",
    "#             model.optimizer_G.step()\n",
    "#             try:\n",
    "#                 model.optimizer_arch.step()\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#         except Exception as e:\n",
    "#             print_timestamped(f\"Error during optimizer step on batch {i}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # accumulate printed losses from model\n",
    "#         losses = model.get_current_losses()\n",
    "#         for k, v in losses.items():\n",
    "#             train_sums[k] += v\n",
    "#         train_batches += 1\n",
    "\n",
    "#         if total_iters % opt.save_latest_freq == 0:\n",
    "#             save_suffix = 'iter_%d' % total_iters if opt.save_by_iter else 'latest'\n",
    "#             model.save_networks(save_suffix)\n",
    "\n",
    "#     # Average training losses\n",
    "#     train_avgs = {k: (train_sums[k] / train_batches if train_batches else 0.0) for k in train_sums}\n",
    "\n",
    "#     # Validation epoch (compute losses without gradient updates)\n",
    "#     model.eval()\n",
    "#     val_sums = collections.defaultdict(float)\n",
    "#     val_batches = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_loader:\n",
    "#             model.set_input(data)\n",
    "#             model.forward()\n",
    "#             # Compute comparable losses using model's criteria\n",
    "#             # GAN loss (generator)\n",
    "#             fake_AB = torch.cat((model.real_A, model.fake_B), 1)\n",
    "#             pred_fake = model.netD(fake_AB)\n",
    "#             loss_G_GAN = model.criterionGAN(pred_fake, True).item()\n",
    "#             # L1 loss on masked region\n",
    "#             loss_G_L1 = model.criterionL1(model.fake_B * model.mask, model.real_B * model.mask).item() * model.opt.lambda_L1\n",
    "#             loss_G_L1 = (loss_G_L1 / torch.sum(model.mask).clamp_min(1)).item()\n",
    "#             # L2 tumor\n",
    "#             loss_G_L2_T = model.criterionTumor(model.fake_B * model.truth, model.real_B * model.truth).item() * model.opt.gamma_TMSE\n",
    "#             loss_G_L2_T = (loss_G_L2_T / torch.sum(model.truth).clamp_min(1)).item()\n",
    "\n",
    "#             # Radiomics loss (recompute same as in model)\n",
    "#             try:\n",
    "#                 rmask = model._radiomics_mask()\n",
    "#                 feats_real = __import__('radiomics.features', fromlist=['masked_tensor_stats']).masked_tensor_stats(model.real_B, rmask)\n",
    "#                 feats_fake = __import__('radiomics.features', fromlist=['masked_tensor_stats']).masked_tensor_stats(model.fake_B, rmask)\n",
    "#                 vec_real = __import__('radiomics.features', fromlist=['features_to_vector']).features_to_vector(feats_real)\n",
    "#                 vec_fake = __import__('radiomics.features', fromlist=['features_to_vector']).features_to_vector(feats_fake)\n",
    "#                 # optional selection\n",
    "#                 selected = getattr(model.opt, 'selected_radiomics', '')\n",
    "#                 if selected:\n",
    "#                     names = sorted(feats_real.keys())\n",
    "#                     keep = [s.strip() for s in selected.split(',') if s.strip()]\n",
    "#                     idx = [i for i, n in enumerate(names) if n in keep]\n",
    "#                     if idx:\n",
    "#                         vec_real = vec_real[:, idx]\n",
    "#                         vec_fake = vec_fake[:, idx]\n",
    "#                 from radiomics.features import normalize_feature_vector\n",
    "#                 vec_real = normalize_feature_vector(vec_real)\n",
    "#                 vec_fake = normalize_feature_vector(vec_fake)\n",
    "#                 loss_G_rad = F.mse_loss(vec_fake, vec_real).item()\n",
    "#             except Exception:\n",
    "#                 loss_G_rad = 0.0\n",
    "\n",
    "#             # Sum up\n",
    "#             val_sums['G_GAN'] += loss_G_GAN\n",
    "#             val_sums['G_L1'] += loss_G_L1\n",
    "#             val_sums['G_L2_T'] += loss_G_L2_T\n",
    "#             val_sums['G_rad'] += loss_G_rad\n",
    "#             val_batches += 1\n",
    "\n",
    "#     val_avgs = {k: (val_sums[k] / val_batches if val_batches else 0.0) for k in val_sums}\n",
    "\n",
    "#     # Print epoch summary\n",
    "#     print_timestamped(f\"Epoch {epoch} train_losses: {train_avgs}\")\n",
    "#     print_timestamped(f\"Epoch {epoch} val_losses: {val_avgs}\")\n",
    "\n",
    "#     model.update_learning_rate()\n",
    "#     if epoch % opt.save_epoch_freq == 0:\n",
    "#         model.save_networks('latest')\n",
    "#         model.save_networks(epoch)\n",
    "\n",
    "# # final save\n",
    "# model.save_networks('final')\n",
    "# print_timestamped('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628be0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(args.csv)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    row['image_nifti']= \"D:/vestibularSchwannoma/AIIMsInfDataset/ImageNiftis/GK2214/vs_gk_0000.nii.gz\"\n",
    "    row['mask_nifti']= \"D:/vestibularSchwannoma/AIIMsInfDataset/Outputs/GK2214/mask.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c57d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_nifti'] = \"D:/vestibularSchwannoma/AIIMsInfDataset/ImageNiftis/GK2214/vs_gk_0000.nii.gz\"\n",
    "df['mask_nifti']  = \"D:/vestibularSchwannoma/AIIMsInfDataset/Outputs/GK2214/mask.nii.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05685595",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"D:\\GenSeg-VS\\GenSeg_VS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37913f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenSeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
